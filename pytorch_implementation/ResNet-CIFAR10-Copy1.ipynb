{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Magikis/project-deep-learning/blob/master/ProjectDeepLearning_pytorchCIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "k5jVWxHwR3LD",
    "outputId": "ca11cfbc-09e9-4545-a9a5-5f25902e5041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['imshow']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "%pylab inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6qzv6D2RSj_O",
    "outputId": "7412f760-8e82-4c83-867a-01ec4ba746a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "means = (0.4914, 0.4822, 0.4465)\n",
    "stds = (0.2023, 0.1994, 0.2010)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='~/datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='~/datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2523
    },
    "colab_type": "code",
    "id": "39GR8FY_Wws2",
    "outputId": "332d6a8d-d513-440c-d20c-09c5a3b053a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5   \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(\n",
    "        np.transpose(npimg, (1, 2, 0))\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "for i in range(3):\n",
    "    images[0, i] *= stds[i]\n",
    "    images[0, i] += means[i]\n",
    "im_to_show = torch.clamp(images[0], -1, 2).permute(1,2,0)\n",
    "\n",
    "plt.imshow(im_to_show)\n",
    "\n",
    "# [classes[x] for x in labels]\n",
    "print(classes[labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Op11PKhXq84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import resnet\n",
    "net = resnet.ResNet18()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCBuvYEycim3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4229
    },
    "colab_type": "code",
    "id": "1OQPZqx_cvN2",
    "outputId": "98d88299-8136-48d0-9b47-ff7ea162de11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    60] loss: 0.065\n",
      "[1,   120] loss: 0.056\n",
      "[1,   180] loss: 0.051\n",
      "[2,    60] loss: 0.046\n",
      "[2,   120] loss: 0.044\n",
      "[2,   180] loss: 0.041\n",
      "[3,    60] loss: 0.039\n",
      "[3,   120] loss: 0.037\n",
      "[3,   180] loss: 0.035\n",
      "[4,    60] loss: 0.033\n",
      "[4,   120] loss: 0.033\n",
      "[4,   180] loss: 0.031\n",
      "[5,    60] loss: 0.030\n",
      "[5,   120] loss: 0.029\n",
      "[5,   180] loss: 0.029\n",
      "[6,    60] loss: 0.027\n",
      "[6,   120] loss: 0.027\n",
      "[6,   180] loss: 0.026\n",
      "[7,    60] loss: 0.025\n",
      "[7,   120] loss: 0.025\n",
      "[7,   180] loss: 0.025\n",
      "[8,    60] loss: 0.023\n",
      "[8,   120] loss: 0.023\n",
      "[8,   180] loss: 0.023\n",
      "[9,    60] loss: 0.022\n",
      "[9,   120] loss: 0.022\n",
      "[9,   180] loss: 0.021\n",
      "[10,    60] loss: 0.020\n",
      "[10,   120] loss: 0.020\n",
      "[10,   180] loss: 0.020\n",
      "[11,    60] loss: 0.019\n",
      "[11,   120] loss: 0.019\n",
      "[11,   180] loss: 0.019\n",
      "[12,    60] loss: 0.018\n",
      "[12,   120] loss: 0.018\n",
      "[12,   180] loss: 0.017\n",
      "[13,    60] loss: 0.017\n",
      "[13,   120] loss: 0.017\n",
      "[13,   180] loss: 0.016\n",
      "[14,    60] loss: 0.016\n",
      "[14,   120] loss: 0.016\n",
      "[14,   180] loss: 0.016\n",
      "[15,    60] loss: 0.016\n",
      "[15,   120] loss: 0.015\n",
      "[15,   180] loss: 0.015\n",
      "[16,    60] loss: 0.014\n",
      "[16,   120] loss: 0.014\n",
      "[16,   180] loss: 0.015\n",
      "[17,    60] loss: 0.014\n",
      "[17,   120] loss: 0.014\n",
      "[17,   180] loss: 0.014\n",
      "[18,    60] loss: 0.014\n",
      "[18,   120] loss: 0.014\n",
      "[18,   180] loss: 0.013\n",
      "[19,    60] loss: 0.013\n",
      "[19,   120] loss: 0.013\n",
      "[19,   180] loss: 0.013\n",
      "[20,    60] loss: 0.012\n",
      "[20,   120] loss: 0.013\n",
      "[20,   180] loss: 0.012\n",
      "[21,    60] loss: 0.012\n",
      "[21,   120] loss: 0.012\n",
      "[21,   180] loss: 0.012\n",
      "[22,    60] loss: 0.011\n",
      "[22,   120] loss: 0.012\n",
      "[22,   180] loss: 0.012\n",
      "[23,    60] loss: 0.011\n",
      "[23,   120] loss: 0.011\n",
      "[23,   180] loss: 0.011\n",
      "[24,    60] loss: 0.011\n",
      "[24,   120] loss: 0.011\n",
      "[24,   180] loss: 0.011\n",
      "[25,    60] loss: 0.011\n",
      "[25,   120] loss: 0.010\n",
      "[25,   180] loss: 0.011\n",
      "[26,    60] loss: 0.010\n",
      "[26,   120] loss: 0.010\n",
      "[26,   180] loss: 0.010\n",
      "[27,    60] loss: 0.010\n",
      "[27,   120] loss: 0.010\n",
      "[27,   180] loss: 0.010\n",
      "[28,    60] loss: 0.009\n",
      "[28,   120] loss: 0.009\n",
      "[28,   180] loss: 0.010\n",
      "[29,    60] loss: 0.009\n",
      "[29,   120] loss: 0.009\n",
      "[29,   180] loss: 0.009\n",
      "[30,    60] loss: 0.009\n",
      "[30,   120] loss: 0.009\n",
      "[30,   180] loss: 0.009\n",
      "[31,    60] loss: 0.009\n",
      "[31,   120] loss: 0.008\n",
      "[31,   180] loss: 0.009\n",
      "[32,    60] loss: 0.008\n",
      "[32,   120] loss: 0.008\n",
      "[32,   180] loss: 0.008\n",
      "[33,    60] loss: 0.008\n",
      "[33,   120] loss: 0.008\n",
      "[33,   180] loss: 0.008\n",
      "[34,    60] loss: 0.008\n",
      "[34,   120] loss: 0.008\n",
      "[34,   180] loss: 0.008\n",
      "[35,    60] loss: 0.007\n",
      "[35,   120] loss: 0.007\n",
      "[35,   180] loss: 0.008\n",
      "[36,    60] loss: 0.007\n",
      "[36,   120] loss: 0.008\n",
      "[36,   180] loss: 0.007\n",
      "[37,    60] loss: 0.007\n",
      "[37,   120] loss: 0.007\n",
      "[37,   180] loss: 0.007\n",
      "[38,    60] loss: 0.007\n",
      "[38,   120] loss: 0.007\n",
      "[38,   180] loss: 0.007\n",
      "[39,    60] loss: 0.007\n",
      "[39,   120] loss: 0.006\n",
      "[39,   180] loss: 0.007\n",
      "[40,    60] loss: 0.007\n",
      "[40,   120] loss: 0.007\n",
      "[40,   180] loss: 0.007\n",
      "Finished traning\n",
      "CPU times: user 19min 24s, sys: 12min 7s, total: 31min 31s\n",
      "Wall time: 31min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(40):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 60 == 59:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished traning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3NIOWMjHen7H",
    "outputId": "21099b8b-1f77-42ff-fc5f-368cb8cad201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "cV0fVX_kfS8m",
    "outputId": "631654f4-da3c-4c86-9c96-68194090f371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 93 %\n",
      "Accuracy of   car : 95 %\n",
      "Accuracy of  bird : 91 %\n",
      "Accuracy of   cat : 85 %\n",
      "Accuracy of  deer : 92 %\n",
      "Accuracy of   dog : 89 %\n",
      "Accuracy of  frog : 96 %\n",
      "Accuracy of horse : 93 %\n",
      "Accuracy of  ship : 96 %\n",
      "Accuracy of truck : 97 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = [0.] * len(classes)\n",
    "class_total = [0.] * len(classes)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "#         c = \n",
    "        for l, p in zip(labels, predicted):\n",
    "            class_correct[l] += int(l == p)\n",
    "            class_total[l] += 1\n",
    "\n",
    "            \n",
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fYauHvDflJn"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b66c5e8dbd96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(testloader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "ProjectDeepLearning_pytorchCIFAR.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
